{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiuser Webchat Prometheus Data Analysis\n",
    "\n",
    "## Test Phases Overview\n",
    "- **Phase 1 (Baseline)**: Normal operation with 100 users, ~0.3 msg/sec/user\n",
    "- **Phase 2 (Throughput Stress)**: 4 levels of message rate escalation (moderate ‚Üí insane)\n",
    "- **Phase 3 (Bandwidth Stress)**: 5 levels of message size escalation (10KB ‚Üí 1000KB)\n",
    "- **Phase 4 (Connection Stress)**: 6 levels of concurrent users (100 ‚Üí 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization style settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Font settings for Korean characters (macOS)\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Prevent minus sign rendering issues\n",
    "\n",
    "# Default figure size\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path configuration\n",
    "BASE_PATH = Path('load_test_results')\n",
    "PHASE_PATHS = {\n",
    "    'phase1_baseline': BASE_PATH / 'phase1_baseline_20251228_194309',\n",
    "    'phase2_throughput': BASE_PATH / 'phase2_throughput_stress_20251228_200708',\n",
    "    'phase3_bandwidth': BASE_PATH / 'phase3_bandwidth_stress_20251228_201715',\n",
    "    'phase4_connection': BASE_PATH / 'phase4_connection_stress_20251228_204327'\n",
    "}\n",
    "\n",
    "def load_phase_data(phase_path):\n",
    "    \"\"\"\n",
    "    Load metrics data for a single test phase.\n",
    "    \n",
    "    Args:\n",
    "        phase_path: Path to the phase directory containing metrics.csv\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with parsed timestamps and elapsed_seconds column\n",
    "    \"\"\"\n",
    "    metrics_file = phase_path / 'metrics.csv'\n",
    "    if not metrics_file.exists():\n",
    "        print(f\"Warning: {metrics_file} not found\")\n",
    "        return None\n",
    "    \n",
    "    # Load CSV with pandas\n",
    "    df = pd.read_csv(metrics_file)\n",
    "    \n",
    "    # Parse timestamp column to datetime objects\n",
    "    # Vector operation: applies pd.to_datetime to entire column at once\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Calculate elapsed time in seconds from start\n",
    "    # Vector operation: (df['timestamp'] - df['timestamp'].iloc[0]) computes time delta for all rows\n",
    "    # .dt.total_seconds() converts timedelta to float seconds\n",
    "    df['elapsed_seconds'] = (df['timestamp'] - df['timestamp'].iloc[0]).dt.total_seconds()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def identify_violations(df, metric, safe_zone_upper):\n",
    "    \"\"\"\n",
    "    Identify when a metric violates its safe zone threshold.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the metric data\n",
    "        metric: Name of the metric column to check\n",
    "        safe_zone_upper: Upper bound threshold for the safe zone\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with violation records (timestamp, metric value, elapsed time)\n",
    "    \"\"\"\n",
    "    # Boolean vector: True where metric exceeds safe zone\n",
    "    # This is a vectorized comparison across entire column\n",
    "    violations = df[df[metric] > safe_zone_upper]\n",
    "    \n",
    "    if len(violations) > 0:\n",
    "        # Select only relevant columns using fancy indexing\n",
    "        return violations[['timestamp', metric, 'elapsed_seconds']]\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Load all phase data\n",
    "print(\"Loading phase data...\")\n",
    "phase_data = {}\n",
    "for phase_name, phase_path in PHASE_PATHS.items():\n",
    "    df = load_phase_data(phase_path)\n",
    "    if df is not None:\n",
    "        phase_data[phase_name] = df\n",
    "        print(f\"  {phase_name}: {len(df)} data points\")\n",
    "\n",
    "# Create convenient references for individual phases\n",
    "baseline_df = phase_data.get('phase1_baseline')\n",
    "throughput_df = phase_data.get('phase2_throughput')\n",
    "bandwidth_df = phase_data.get('phase3_bandwidth')\n",
    "connection_df = phase_data.get('phase4_connection')\n",
    "\n",
    "print(f\"\\nLoaded {len(phase_data)} phases successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Collected Metrics Description\n",
    "\n",
    "Documentation of Prometheus metrics collected from the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric descriptions  \n",
    "**Timestamp**  \n",
    "timestamp: Metric collection time (UTC)  \n",
    "\n",
    "**Connection metrics**  \n",
    "connected_users: Number of currently connected WebSocket clients  \n",
    "conn_attempt_rate: Connection attempt rate per second  \n",
    "conn_success_rate: Connection success rate (successful/total attempts)  \n",
    "conn_fail_rate: Connection failure rate per second  \n",
    "disconn_rate: Disconnection rate per second  \n",
    "\n",
    "**Message processing**  \n",
    "message_rate: Messages processed per second (msg/s)  \n",
    "\n",
    "**Latency metrics (milliseconds)**  \n",
    "e2e_latency_p95: End-to-end latency 95th percentile - client send to all clients receive (ms)  \n",
    "e2e_latency_p99: End-to-end latency 99th percentile (ms)  \n",
    "\n",
    "**Event loop and Redis metrics**  \n",
    "eventloop_lag_p95: Event loop lag 95th percentile - blocking operation indicator (ms)  \n",
    "eventloop_lag_p99: Event loop lag 99th percentile (ms)  \n",
    "redis_stream_lag: Redis Stream consumer lag - delay in message consumption  \n",
    "redis_op_latency_p95: Redis operation latency 95th percentile (ms)  \n",
    "\n",
    "**System resources**  \n",
    "memory_bytes: Memory usage in bytes  \n",
    "\n",
    "**Error metrics**  \n",
    "error_rate: Error occurrence rate per second  \n",
    " \n",
    "**Derived metrics**\n",
    "elapsed_seconds: Seconds elapsed since test start  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis (EDA)\n",
    "\n",
    "Examine missing values and basic statistics for each phase's collected metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Phase-wise Metrics EDA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for phase_name, df in phase_data.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"\\n1. Basic Info:\")\n",
    "    print(f\"   - Data points: {len(df)}\")\n",
    "    print(f\"   - Columns: {len(df.columns)}\")\n",
    "    print(f\"   - Duration: {df['elapsed_seconds'].max():.1f}s ({df['elapsed_seconds'].max()/60:.1f}min)\")\n",
    "    \n",
    "    # Missing value analysis\n",
    "    # Vector operation: isnull() returns boolean DataFrame, sum() aggregates along columns\n",
    "    missing = df.isnull().sum()\n",
    "    # Vector operation: divide entire Series by scalar (len(df)) and multiply by 100\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    # Boolean indexing: select only columns where percentage > 0\n",
    "    missing_cols = missing_pct[missing_pct > 0]\n",
    "    \n",
    "    print(f\"\\n2. Missing Values:\")\n",
    "    if len(missing_cols) > 0:\n",
    "        for col, pct in missing_cols.items():\n",
    "            print(f\"   - {col}: {missing[col]} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   - No missing values\")\n",
    "    \n",
    "    # Numeric columns summary statistics\n",
    "    # Vector operation: select_dtypes returns subset of columns by data type\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    print(f\"\\n3. Numeric Metrics:\")\n",
    "    print(f\"   - Count: {len(numeric_cols)}\")\n",
    "    print(f\"   - Columns: {', '.join(list(numeric_cols)[:5])}...\")\n",
    "    \n",
    "    # Display summary statistics for key metrics\n",
    "    # Vector operation: describe() computes count, mean, std, min, quartiles, max for all numeric columns\n",
    "    key_metrics = ['connected_users', 'message_rate', 'e2e_latency_p95', 'error_rate']\n",
    "    # List comprehension with conditional: filter metrics that exist in the DataFrame\n",
    "    available_metrics = [m for m in key_metrics if m in df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        print(f\"\\n4. Key Metrics Summary:\")\n",
    "        # Select multiple columns at once and compute statistics\n",
    "        summary = df[available_metrics].describe()\n",
    "        print(summary.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Phase Comparison Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leading Indicator Analysis via Lagged Correlation\n",
    "\n",
    "Define a function to find metrics that change before errors/failures occur (leading indicators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lagged_correlations(df, target_col, candidate_metrics, lags=[0, 5, 10, 15, 20, 30]):\n",
    "    \"\"\"\n",
    "    Calculate lagged correlations to identify leading indicators (optimized with vector operations).\n",
    "    \n",
    "    This function computes Pearson correlation between a target metric (e.g., error_rate) and\n",
    "    candidate metrics at various time lags. A positive lag means the candidate metric changes\n",
    "    *before* the target metric, making it a potential leading indicator.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing time series data\n",
    "        target_col: Target metric column name (e.g., 'error_rate', 'failure_count')\n",
    "        candidate_metrics: List of candidate metric column names to test\n",
    "        lags: List of time lags in seconds to test (default: [0, 5, 10, 15, 20, 30])\n",
    "    \n",
    "    Returns:\n",
    "        corr_matrix: DataFrame with metrics as rows and lags as columns, values are correlations\n",
    "    \n",
    "    Vector Operations Used:\n",
    "        - df[metric].shift(lag): Shifts entire column backward by 'lag' positions\n",
    "        - df[target_col].corr(shifted): Computes Pearson correlation between two Series\n",
    "        - pd.DataFrame(data, index, columns): Constructs DataFrame from nested dict\n",
    "    \"\"\"\n",
    "    # Dictionary to store correlation values: {metric_name: {lag: correlation}}\n",
    "    corr_data = {}\n",
    "    \n",
    "    # Iterate through each candidate metric\n",
    "    for metric in candidate_metrics:\n",
    "        if metric not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Store correlations at different lags for this metric\n",
    "        corr_data[metric] = {}\n",
    "        \"\"\" \n",
    "        # Test each time lag\n",
    "        for lag in lags:\n",
    "            # Shift metric backward by 'lag' positions\n",
    "            # This makes the metric's earlier values align with target's later values\n",
    "            # Vector operation: shift() operates on entire column without loops\n",
    "            shifted_metric = df[metric].shift(lag)\n",
    "            \n",
    "            # Calculate Pearson correlation between shifted metric and target\n",
    "            # Vector operation: corr() computes correlation using vectorized numpy operations\n",
    "            # Only uses rows where both values are non-null\n",
    "            correlation = df[target_col].corr(shifted_metric)\n",
    "            \n",
    "            # Store the correlation value\n",
    "            corr_data[metric][lag] = correlation\n",
    "        \"\"\"\n",
    "    # Convert nested dictionary to DataFrame\n",
    "    # Rows = metrics, Columns = lags, Values = correlations\n",
    "    corr_matrix = pd.DataFrame(corr_data).T\n",
    "    \n",
    "    # Sort by absolute correlation at lag=0 (descending)\n",
    "    # Vector operation: abs() applied to entire column, sort_values() reorders rows\n",
    "    if 0 in lags:\n",
    "        corr_matrix = corr_matrix.sort_values(by=0, key=abs, ascending=False) # ===>.abs dfÏóê Î∞îÎ°ú map key ÎßêÍ≥† \n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "def find_top_leading_indicators(corr_matrix, top_n=3):\n",
    "    \"\"\"\n",
    "    Identify top leading indicators from lagged correlation matrix.\n",
    "    \n",
    "    Args:\n",
    "        corr_matrix: DataFrame from calculate_lagged_correlations()\n",
    "        top_n: Number of top indicators to return\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples: (metric_name, optimal_lag, correlation_value)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for metric in corr_matrix.index:\n",
    "        # Vector operation: abs() on entire row, idxmax() finds column with max value\n",
    "        row = corr_matrix.loc[metric]\n",
    "        if row.isna().all():\n",
    "            continue\n",
    "\n",
    "        # skipna=True ensures we skip NaN values\n",
    "        optimal_lag = row.abs().idxmax(skipna=True)\n",
    "        if pd.isna(optimal_lag):\n",
    "            continue\n",
    "\n",
    "        max_corr = corr_matrix.loc[metric, optimal_lag]\n",
    "        results.append((metric, optimal_lag, max_corr))\n",
    "    \n",
    "    # Sort by absolute correlation value (descending)\n",
    "    results.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    return results[:top_n]\n",
    "\n",
    "def find_top_predictors(corr_matrix, top_n=3):\n",
    "    \"\"\"\n",
    "    ÏµúÍ≥† ÏòàÏ∏° ÏßÄÌëú Ï∞æÍ∏∞\n",
    "\n",
    "    Args:\n",
    "        corr_matrix: DataFrame from calculate_lagged_correlations()\n",
    "                    (rows=metrics, columns=lags)\n",
    "        top_n: Number of top indicators to return\n",
    "\n",
    "    Returns:\n",
    "        List of dicts: [{'metric': name, 'lag': lag, 'correlation': value}, ...]\n",
    "\n",
    "    Î≤°ÌÑ∞ Ïó∞ÏÇ∞ ÏÇ¨Ïö©:\n",
    "    - corr_matrix.abs().max(axis=1) : Í∞Å Î©îÌä∏Î¶≠(Ìñâ)Ïùò ÏµúÎåÄ Ï†àÎåÄÍ∞í ÏÉÅÍ¥ÄÍ≥ÑÏàò\n",
    "    - corr_matrix.loc[metric].abs().idxmax() : ÏµúÎåÄÍ∞íÏùò Ïù∏Îç±Ïä§(lag) Ï∞æÍ∏∞\n",
    "    \"\"\"\n",
    "    if corr_matrix is None or corr_matrix.empty:\n",
    "        return []\n",
    "\n",
    "    # Í∞Å Î©îÌä∏Î¶≠Î≥Ñ ÏµúÎåÄ ÏÉÅÍ¥ÄÍ≥ÑÏàò (Î≤°ÌÑ∞ Ïó∞ÏÇ∞: axis=1Î°ú Í∞Å ÌñâÏùò ÏµúÎåÄÍ∞í)\n",
    "    max_corrs = corr_matrix.abs().max(axis=1)\n",
    "\n",
    "    # NaN Ï†úÍ±∞\n",
    "    max_corrs = max_corrs.dropna()\n",
    "\n",
    "    if max_corrs.empty:\n",
    "        return []\n",
    "\n",
    "    # ÏÉÅÏúÑ NÍ∞ú Î©îÌä∏Î¶≠ ÏÑ†ÌÉù (Î≤°ÌÑ∞ Ïó∞ÏÇ∞: sort_values)\n",
    "    top_metrics = max_corrs.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    # Í∞Å Î©îÌä∏Î¶≠Ïùò ÏµúÏ†Å lag Ï∞æÍ∏∞\n",
    "    predictors = []\n",
    "    for metric in top_metrics.index:\n",
    "        # Skip rows where all values are NaN\n",
    "        row = corr_matrix.loc[metric]\n",
    "        if row.isna().all():\n",
    "            continue\n",
    "\n",
    "        # Î≤°ÌÑ∞ Ïó∞ÏÇ∞: idxmax()Î°ú ÏµúÎåÄ ÏÉÅÍ¥ÄÍ≥ÑÏàòÏùò lag Ï∞æÍ∏∞\n",
    "        best_lag = row.abs().idxmax(skipna=True)\n",
    "\n",
    "        # Check if best_lag is NaN\n",
    "        if pd.isna(best_lag):\n",
    "            continue\n",
    "\n",
    "        best_corr = corr_matrix.loc[metric, best_lag]\n",
    "\n",
    "        predictors.append({\n",
    "            'metric': metric,\n",
    "            'lag': best_lag,\n",
    "            'correlation': best_corr\n",
    "        })\n",
    "\n",
    "    return predictors\n",
    "    \n",
    "def plot_correlation_heatmap(corr_matrix, title=\"Lagged Correlation Heatmap\", figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Visualize lagged correlation matrix as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        corr_matrix: DataFrame from calculate_lagged_correlations()\n",
    "        title: Plot title\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap with diverging colormap (red-white-blue)\n",
    "    # vmin=-1, vmax=1 ensures proper scaling for correlation values\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "                center=0, vmin=-1, vmax=1, \n",
    "                cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Lag (seconds)', fontsize=12)\n",
    "    plt.ylabel('Metrics', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Leading indicator analysis functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core metrics for analysis\n",
    "CORE_METRICS = [\n",
    "    ('e2e_latency_p95', 'E2E Latency P95 (ms)'),\n",
    "    ('eventloop_lag_p95', 'Event Loop Lag P95 (ms)'),\n",
    "    ('memory_bytes', 'Memory Usage (MB)'),\n",
    "    ('error_rate', 'Error Rate (%)')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "# Flatten 2D array of axes to 1D for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = {\n",
    "    'phase1_baseline': '#2ecc71',\n",
    "    'phase2_throughput': '#3498db',\n",
    "    'phase3_bandwidth': '#f39c12',\n",
    "    'phase4_connection': '#e74c3c'\n",
    "}\n",
    "\n",
    "# Iterate through core metrics and plot on subplots\n",
    "for idx, (metric, label) in enumerate(CORE_METRICS):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for phase_name, df in phase_data.items():\n",
    "        if metric in df.columns:\n",
    "            # Convert memory from bytes to MB for readability\n",
    "            # Vector operation: divide entire column by scalar (1024*1024)\n",
    "            y_data = df[metric] / (1024*1024) if metric == 'memory_bytes' else df[metric]\n",
    "            \n",
    "            # Plot time series with phase-specific color\n",
    "            ax.plot(df['elapsed_seconds'], y_data, \n",
    "                   label=phase_name.replace('_', ' ').title(),\n",
    "                   color=colors[phase_name], alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Elapsed Time (seconds)', fontsize=10)\n",
    "    ax.set_ylabel(label, fontsize=10)\n",
    "    ax.set_title(f'{label} Across All Phases', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Core Metrics Comparison Across Test Phases', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 1: Baseline Analysis\n",
    "\n",
    "### Objective\n",
    "Establish normal operating conditions and define **Safe Zones** for key metrics.\n",
    "\n",
    "### Safe Zone Definition\n",
    "For each metric, the safe zone upper bound is calculated as:\n",
    "```\n",
    "Safe Zone Upper = Mean + (2 √ó Standard Deviation)\n",
    "```\n",
    "This represents approximately 95% of normal operation data points, assuming normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Safe Zones from baseline data\n",
    "safe_zones = {}\n",
    "for metric, _ in CORE_METRICS:\n",
    "    if metric in baseline_df.columns:\n",
    "        # Vector operations: mean() and std() compute statistics across entire column\n",
    "        # These operations process all data points at once (no loops)\n",
    "        mean_val = baseline_df[metric].mean()\n",
    "        std_val = baseline_df[metric].std()\n",
    "        \n",
    "        # Safe zone: mean + 2 standard deviations (covers ~95% of normal data)\n",
    "        safe_zones[metric] = mean_val + (2 * std_val)\n",
    "        \n",
    "        print(f\"{metric:20s}: mean={mean_val:8.2f}, std={std_val:8.2f}, \"\n",
    "              f\"safe_zone_upper={safe_zones[metric]:8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Level Mapping Functions\n",
    "\n",
    "Map elapsed time to actual load levels (message rate, message size, user count) for each phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_rate_phase2(elapsed_sec):\n",
    "    \"\"\"\n",
    "    Map elapsed time to message rate for Phase 2 (Throughput Stress).\n",
    "    \n",
    "    Phase 2 escalates message rate every 2 minutes:\n",
    "    - 0-120s: 70 msg/s (moderate)\n",
    "    - 120-240s: 200 msg/s (high)\n",
    "    - 240-360s: 500 msg/s (very high)\n",
    "    - 360+s: 1000 msg/s (extreme)\n",
    "    \n",
    "    Args:\n",
    "        elapsed_sec: Seconds elapsed since test start (scalar or array)\n",
    "        \n",
    "    Returns:\n",
    "        Message rate (msg/s)\n",
    "    \"\"\"\n",
    "    # Use numpy vectorized conditionals for efficient array operations\n",
    "    # np.where works element-wise on arrays without explicit loops\n",
    "    if isinstance(elapsed_sec, (int, float)):\n",
    "        if elapsed_sec < 120:\n",
    "            return 70\n",
    "        elif elapsed_sec < 240:\n",
    "            return 200\n",
    "        elif elapsed_sec < 360:\n",
    "            return 500\n",
    "        else:\n",
    "            return 1000\n",
    "    else:\n",
    "        # Vectorized version for pandas Series or numpy array\n",
    "        # Nested np.where evaluates conditions element-wise\n",
    "        return np.where(elapsed_sec < 120, 70,\n",
    "               np.where(elapsed_sec < 240, 200,\n",
    "               np.where(elapsed_sec < 360, 500, 1000)))\n",
    "\n",
    "def get_message_size_phase3(elapsed_sec):\n",
    "    \"\"\"\n",
    "    Map elapsed time to message size for Phase 3 (Bandwidth Stress).\n",
    "    \n",
    "    Phase 3 escalates message size every 2 minutes:\n",
    "    - 0-120s: 10 KB\n",
    "    - 120-240s: 50 KB\n",
    "    - 240-360s: 100 KB\n",
    "    - 360-480s: 500 KB\n",
    "    - 480+s: 1000 KB (1 MB)\n",
    "    \"\"\"\n",
    "    if isinstance(elapsed_sec, (int, float)):\n",
    "        if elapsed_sec < 120:\n",
    "            return 10\n",
    "        elif elapsed_sec < 240:\n",
    "            return 50\n",
    "        elif elapsed_sec < 360:\n",
    "            return 100\n",
    "        elif elapsed_sec < 480:\n",
    "            return 500\n",
    "        else:\n",
    "            return 1000\n",
    "    else:\n",
    "        return np.where(elapsed_sec < 120, 10,\n",
    "               np.where(elapsed_sec < 240, 50,\n",
    "               np.where(elapsed_sec < 360, 100,\n",
    "               np.where(elapsed_sec < 480, 500, 1000))))\n",
    "\n",
    "def get_user_count_phase4(elapsed_sec):\n",
    "    \"\"\"\n",
    "    Map elapsed time to user count for Phase 4 (Connection Stress).\n",
    "    \n",
    "    Phase 4 escalates concurrent users every 2 minutes:\n",
    "    - 0-120s: 100 users\n",
    "    - 120-240s: 200 users\n",
    "    - 240-360s: 300 users\n",
    "    - 360-480s: 400 users\n",
    "    - 480-600s: 500 users\n",
    "    - 600+s: 600 users\n",
    "    \"\"\"\n",
    "    if isinstance(elapsed_sec, (int, float)):\n",
    "        if elapsed_sec < 120:\n",
    "            return 100\n",
    "        elif elapsed_sec < 240:\n",
    "            return 200\n",
    "        elif elapsed_sec < 360:\n",
    "            return 300\n",
    "        elif elapsed_sec < 480:\n",
    "            return 400\n",
    "        elif elapsed_sec < 600:\n",
    "            return 500\n",
    "        else:\n",
    "            return 600\n",
    "    else:\n",
    "        return np.where(elapsed_sec < 120, 100,\n",
    "               np.where(elapsed_sec < 240, 200,\n",
    "               np.where(elapsed_sec < 360, 300,\n",
    "               np.where(elapsed_sec < 480, 400,\n",
    "               np.where(elapsed_sec < 600, 500, 600)))))\n",
    "\n",
    "print(\"Load level mapping functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 2: Throughput Stress Analysis\n",
    "\n",
    "### Scenario\n",
    "- Message rate escalation every 2 minutes: 70 ‚Üí 200 ‚Üí 500 ‚Üí 1000 msg/s\n",
    "- Objective: Identify message rate threshold where system performance degrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_df = phase_data['phase2_throughput']\n",
    "\n",
    "# Visualize safe zone violations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, label) in enumerate(CORE_METRICS):\n",
    "    if metric not in phase2_df.columns:\n",
    "        continue\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Convert memory to MB if needed\n",
    "    # Vector operation: conditional division applied to entire column\n",
    "    y_data = phase2_df[metric] / (1024*1024) if metric == 'memory_bytes' else phase2_df[metric]\n",
    "    safe_zone = safe_zones[metric] / (1024*1024) if metric == 'memory_bytes' else safe_zones[metric]\n",
    "    \n",
    "    # Plot metric over time\n",
    "    ax.plot(phase2_df['elapsed_seconds'], y_data, \n",
    "           label=f'{label}', color='#3498db', linewidth=2)\n",
    "    \n",
    "    # Draw safe zone threshold line\n",
    "    ax.axhline(y=safe_zone, color='red', linestyle='--', linewidth=2,\n",
    "              label=f'Safe Zone Upper ({safe_zone:.1f})')\n",
    "    \n",
    "    # Highlight violation area\n",
    "    # Vector operation: boolean masking to select rows where metric exceeds safe zone\n",
    "    violations = y_data > safe_zone\n",
    "    if violations.any():\n",
    "        # Fill area where violations occur\n",
    "        ax.fill_between(phase2_df['elapsed_seconds'], y_data, safe_zone,\n",
    "                        where=violations, alpha=0.3, color='red',\n",
    "                        label='Violation Zone')\n",
    "    \n",
    "    ax.set_xlabel('Elapsed Time (seconds)', fontsize=10)\n",
    "    ax.set_ylabel(label, fontsize=10)\n",
    "    ax.set_title(f'Phase 2: {label} vs Safe Zone', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Phase 2 (Throughput Stress): Safe Zone Violations', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Message Rate Analysis at Violation Points\n",
    "\n",
    "Analyze the message rate when E2E Latency P95 first violates the safe zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze E2E Latency P95 violations\n",
    "metric = 'e2e_latency_p95'\n",
    "violations_p2 = identify_violations(phase2_df, metric, safe_zones[metric])\n",
    "\n",
    "if len(violations_p2) > 0:\n",
    "    # Get first violation\n",
    "    first_violation = violations_p2.iloc[0]\n",
    "    first_violation_time = first_violation['elapsed_seconds']\n",
    "    first_violation_value = first_violation[metric]\n",
    "    \n",
    "    # Calculate message rate at violation time\n",
    "    msg_rate_at_violation = get_message_rate_phase2(first_violation_time)\n",
    "    \n",
    "    print(\"E2E Latency P95 Safe Zone Violation Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Safe Zone Upper Bound: {safe_zones[metric]:.2f} ms\")\n",
    "    print(f\"First Violation Time: {first_violation_time:.1f}s\")\n",
    "    print(f\"First Violation Value: {first_violation_value:.2f} ms\")\n",
    "    print(f\"Message Rate at Violation: {msg_rate_at_violation} msg/s\")\n",
    "    print(f\"Total Violations: {len(violations_p2)}\")\n",
    "    print(f\"\\nConclusion: System performance degrades when message rate exceeds {msg_rate_at_violation} msg/s\")\n",
    "else:\n",
    "    print(\"No safe zone violations detected for E2E Latency P95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Leading Indicator Lagged Correlation Analysis\n",
    "\n",
    "Find metrics that change before error_rate increases, enabling predictive monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 leading indicator analysis\n",
    "target_metric_p2 = 'error_rate'\n",
    "\n",
    "# Candidate metrics (exclude error_rate itself)\n",
    "# List comprehension: filter numeric columns, exclude target and timestamp columns\n",
    "candidate_metrics_p2 = [\n",
    "    col for col in phase2_df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in ['error_rate', 'failure_count', 'elapsed_seconds', 'timestamp']\n",
    "]\n",
    "\n",
    "# Calculate lagged correlations\n",
    "corr_matrix_p2 = calculate_lagged_correlations(\n",
    "    phase2_df, target_metric_p2, candidate_metrics_p2, \n",
    "    lags=[0, 5, 10, 15, 20, 30]\n",
    ")\n",
    "\n",
    "# Display top 10 metrics with strongest correlation at any lag\n",
    "print(\"Phase 2: Top 10 Metrics with Lagged Correlation to Error Rate\")\n",
    "print(\"=\" * 80)\n",
    "print(corr_matrix_p2.head(10).round(3))\n",
    "\n",
    "# Find top leading indicators\n",
    "top_indicators_p2 = find_top_leading_indicators(corr_matrix_p2, top_n=3)\n",
    "\n",
    "print(f\"\\n\\nTop 3 Leading Indicators for Error Rate:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (metric, lag, corr) in enumerate(top_indicators_p2, 1):\n",
    "    print(f\"{i}. {metric:30s} | Optimal Lag: {lag:2d}s | Correlation: {corr:+.3f}\")\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plot_correlation_heatmap(corr_matrix_p2.head(15), \n",
    "                        title=\"Phase 2: Lagged Correlation with Error Rate (Top 15 Metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between message rate and core metrics\n",
    "correlation_metrics = ['message_rate', 'e2e_latency_p95', 'eventloop_lag_p95', \n",
    "                      'memory_bytes', 'error_rate']\n",
    "\n",
    "# Filter to metrics that exist in the DataFrame\n",
    "available_metrics = [m for m in correlation_metrics if m in phase2_df.columns]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "# Vector operation: corr() computes pairwise correlations for all column combinations\n",
    "corr_matrix = phase2_df[available_metrics].corr()\n",
    "\n",
    "print(\"Phase 2: Correlation Matrix\")\n",
    "print(\"=\" * 80)\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Visualize correlation matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "           center=0, vmin=-1, vmax=1, square=True,\n",
    "           cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Phase 2: Metric Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Strong Correlation Visualization\n",
    "\n",
    "Visualize metric pairs with correlation coefficient >= 0.7 to identify metrics that move together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find strongly correlated metric pairs (|correlation| >= 0.7)\n",
    "strong_corrs_p2 = []\n",
    "for i in range(len(available_metrics)):\n",
    "    for j in range(i+1, len(available_metrics)):\n",
    "        metric1 = available_metrics[i]\n",
    "        metric2 = available_metrics[j]\n",
    "        corr_val = corr_matrix.loc[metric1, metric2]\n",
    "        \n",
    "        # Check if absolute correlation exceeds threshold\n",
    "        if abs(corr_val) >= 0.7:\n",
    "            strong_corrs_p2.append((metric1, metric2, corr_val))\n",
    "\n",
    "if strong_corrs_p2:\n",
    "    print(f\"Found {len(strong_corrs_p2)} strongly correlated pairs (|r| >= 0.7)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Plot all strong correlations on same figure\n",
    "    fig, axes = plt.subplots(len(strong_corrs_p2), 1, \n",
    "                            figsize=(14, 5*len(strong_corrs_p2)))\n",
    "    \n",
    "    # Ensure axes is always iterable\n",
    "    if len(strong_corrs_p2) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (m1, m2, corr) in enumerate(strong_corrs_p2):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create dual y-axis plot\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        # Plot both metrics\n",
    "        line1 = ax.plot(phase2_df['elapsed_seconds'], phase2_df[m1], \n",
    "                       'b-', label=m1, linewidth=2)\n",
    "        line2 = ax2.plot(phase2_df['elapsed_seconds'], phase2_df[m2], \n",
    "                        'r-', label=m2, linewidth=2)\n",
    "        \n",
    "        # Labels and title\n",
    "        ax.set_xlabel('Elapsed Time (seconds)', fontsize=11)\n",
    "        ax.set_ylabel(m1, color='b', fontsize=11)\n",
    "        ax2.set_ylabel(m2, color='r', fontsize=11)\n",
    "        ax.set_title(f'{m1} vs {m2} (correlation: {corr:+.3f})', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Combine legends\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax.legend(lines, labels, loc='upper left')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        print(f\"{idx+1}. {m1:30s} <-> {m2:30s} | r = {corr:+.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No strongly correlated pairs found (threshold: |r| >= 0.7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 3: Bandwidth Stress Analysis\n",
    "\n",
    "### Scenario\n",
    "- Message size escalation every 2 minutes: 10 ‚Üí 50 ‚Üí 100 ‚Üí 500 ‚Üí 1000 KB\n",
    "- Objective: Identify message size threshold where bandwidth becomes bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase3_df = phase_data['phase3_bandwidth']\n",
    "\n",
    "# Visualize safe zone violations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, label) in enumerate(CORE_METRICS):\n",
    "    if metric not in phase3_df.columns:\n",
    "        continue\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Convert memory to MB if needed\n",
    "    y_data = phase3_df[metric] / (1024*1024) if metric == 'memory_bytes' else phase3_df[metric]\n",
    "    safe_zone = safe_zones[metric] / (1024*1024) if metric == 'memory_bytes' else safe_zones[metric]\n",
    "    \n",
    "    # Plot metric over time\n",
    "    ax.plot(phase3_df['elapsed_seconds'], y_data, \n",
    "           label=f'{label}', color='#f39c12', linewidth=2)\n",
    "    \n",
    "    # Draw safe zone threshold\n",
    "    ax.axhline(y=safe_zone, color='red', linestyle='--', linewidth=2,\n",
    "              label=f'Safe Zone Upper ({safe_zone:.1f})')\n",
    "    \n",
    "    # Highlight violations\n",
    "    violations = y_data > safe_zone\n",
    "    if violations.any():\n",
    "        ax.fill_between(phase3_df['elapsed_seconds'], y_data, safe_zone,\n",
    "                        where=violations, alpha=0.3, color='red',\n",
    "                        label='Violation Zone')\n",
    "    \n",
    "    ax.set_xlabel('Elapsed Time (seconds)', fontsize=10)\n",
    "    ax.set_ylabel(label, fontsize=10)\n",
    "    ax.set_title(f'Phase 3: {label} vs Safe Zone', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Phase 3 (Bandwidth Stress): Safe Zone Violations', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Message Size Analysis at Violation Points\n",
    "\n",
    "Analyze message size when E2E Latency P95, Memory Usage, and Error Rate violate safe zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple metric violations for Phase 3\n",
    "metrics_to_check = [\n",
    "    ('e2e_latency_p95', 'E2E Latency P95'),\n",
    "    ('memory_bytes', 'Memory Usage'),\n",
    "    ('error_rate', 'Error Rate')\n",
    "]\n",
    "\n",
    "print(\"Phase 3: Multi-Metric Violation Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric, label in metrics_to_check:\n",
    "    if metric not in phase3_df.columns or metric not in safe_zones:\n",
    "        continue\n",
    "    \n",
    "    violations = identify_violations(phase3_df, metric, safe_zones[metric])\n",
    "    \n",
    "    if len(violations) > 0:\n",
    "        first_violation = violations.iloc[0]\n",
    "        violation_time = first_violation['elapsed_seconds']\n",
    "        violation_value = first_violation[metric]\n",
    "        msg_size = get_message_size_phase3(violation_time)\n",
    "        \n",
    "        # Convert memory to MB for display\n",
    "        if metric == 'memory_bytes':\n",
    "            violation_value_display = violation_value / (1024*1024)\n",
    "            safe_zone_display = safe_zones[metric] / (1024*1024)\n",
    "            unit = 'MB'\n",
    "        else:\n",
    "            violation_value_display = violation_value\n",
    "            safe_zone_display = safe_zones[metric]\n",
    "            unit = 'ms' if 'latency' in metric else '%' if 'rate' in metric else ''\n",
    "        \n",
    "        print(f\"\\n{label}:\")\n",
    "        print(f\"  Safe Zone Upper: {safe_zone_display:.2f} {unit}\")\n",
    "        print(f\"  First Violation: {violation_time:.1f}s at {violation_value_display:.2f} {unit}\")\n",
    "        print(f\"  Message Size: {msg_size} KB\")\n",
    "        print(f\"  Total Violations: {len(violations)}\")\n",
    "    else:\n",
    "        print(f\"\\n{label}: No violations detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Leading Indicator Lagged Correlation Analysis\n",
    "\n",
    "Find metrics that change before error_rate increases under bandwidth stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 leading indicator analysis\n",
    "target_metric_p3 = 'error_rate'\n",
    "\n",
    "candidate_metrics_p3 = [\n",
    "    col for col in phase3_df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in ['error_rate', 'failure_count', 'elapsed_seconds', 'timestamp']\n",
    "]\n",
    "\n",
    "# Calculate lagged correlations\n",
    "corr_matrix_p3 = calculate_lagged_correlations(\n",
    "    phase3_df, target_metric_p3, candidate_metrics_p3, \n",
    "    lags=[0, 5, 10, 15, 20, 30]\n",
    ")\n",
    "\n",
    "print(\"Phase 3: Top 10 Metrics with Lagged Correlation to Error Rate\")\n",
    "print(\"=\" * 80)\n",
    "print(corr_matrix_p3.head(10).round(3))\n",
    "\n",
    "# Find top leading indicators\n",
    "top_indicators_p3 = find_top_leading_indicators(corr_matrix_p3, top_n=3)\n",
    "\n",
    "print(f\"\\n\\nTop 3 Leading Indicators for Error Rate:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (metric, lag, corr) in enumerate(top_indicators_p3, 1):\n",
    "    print(f\"{i}. {metric:30s} | Optimal Lag: {lag:2d}s | Correlation: {corr:+.3f}\")\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plot_correlation_heatmap(corr_matrix_p3.head(15), \n",
    "                        title=\"Phase 3: Lagged Correlation with Error Rate (Top 15 Metrics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Phase 3 Locust stats_history data\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "phase3_stats_files = glob.glob(str(PHASE_PATHS['phase3_bandwidth'] / '*_stats_history.csv'))\n",
    "\n",
    "if phase3_stats_files:\n",
    "    print(f\"Found {len(phase3_stats_files)} stats files\")\n",
    "    \n",
    "    # Load and combine all stats files\n",
    "    stats_dfs = []\n",
    "    for f in phase3_stats_files:\n",
    "        df = pd.read_csv(f)\n",
    "        stats_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    # Vector operation: concat combines multiple DataFrames efficiently\n",
    "    phase3_stats = pd.concat(stats_dfs, ignore_index=True)\n",
    "    \n",
    "    # Parse timestamp\n",
    "    phase3_stats['Timestamp'] = pd.to_datetime(phase3_stats['Timestamp'], unit='s')\n",
    "    \n",
    "    print(f\"Loaded {len(phase3_stats)} stats records\")\n",
    "    print(f\"Columns: {list(phase3_stats.columns)}\")\n",
    "else:\n",
    "    print(\"No stats_history files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failureÎ°ú Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞ \n",
    "# Phase 3 Locust stats_history Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "phase3_stats_files = glob.glob(str(PHASE_PATHS['phase3_bandwidth'] / '*_stats_history.csv'))\n",
    "\n",
    "# Î™®Îì† stats_history ÌååÏùº Î°úÎìú Î∞è Î≥ëÌï© (Î≤°ÌÑ∞ Ïó∞ÏÇ∞)\n",
    "stats_dfs = []\n",
    "for file in phase3_stats_files:\n",
    "  df = pd.read_csv(file)\n",
    "  # TimestampÎ•º datetimeÏúºÎ°ú Î≥ÄÌôò (Î≤°ÌÑ∞ Ïó∞ÏÇ∞)\n",
    "  df['timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "  stats_dfs.append(df)\n",
    "\n",
    "# Î™®Îì† DataFrame Î≥ëÌï© (Î≤°ÌÑ∞ Ïó∞ÏÇ∞: concat)\n",
    "stats_combined = pd.concat(stats_dfs, ignore_index=True)\n",
    "\n",
    "# ÏãúÍ∞ÑÏàú Ï†ïÎ†¨ (Î≤°ÌÑ∞ Ïó∞ÏÇ∞: sort_values)\n",
    "stats_combined = stats_combined.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Í≤ΩÍ≥º ÏãúÍ∞Ñ Í≥ÑÏÇ∞ (Î≤°ÌÑ∞ Ïó∞ÏÇ∞)\n",
    "stats_combined['elapsed_seconds'] = (stats_combined['timestamp'] -\n",
    "                                   stats_combined['timestamp'].min()).dt.total_seconds()\n",
    "\n",
    "print(f\"‚úì ÌÜµÌï© Îç∞Ïù¥ÌÑ∞: {len(stats_combined)}Í∞ú ÏÉòÌîå\")\n",
    "print(f\"  ‚Ä¢ ÏàòÏßë Í∏∞Í∞Ñ: {stats_combined['elapsed_seconds'].max():.1f}Ï¥à\")\n",
    "print(f\"  ‚Ä¢ Failures/s Î≤îÏúÑ: [{stats_combined['Failures/s'].min():.2f}, \"\n",
    "    f\"{stats_combined['Failures/s'].max():.2f}]\")\n",
    "print(f\"  ‚Ä¢ Total Failure Count: {stats_combined['Total Failure Count'].max():.0f}Í±¥\")\n",
    "\n",
    "# Failures/sÍ∞Ä 0Î≥¥Îã§ ÌÅ∞ ÏÉòÌîåÎßå ÌôïÏù∏\n",
    "failure_samples = stats_combined[stats_combined['Failures/s'] > 0]\n",
    "print(f\"  ‚Ä¢ Ïã§Ìå® Î∞úÏÉù ÏÉòÌîå: {len(failure_samples)}Í∞ú ({len(failure_samples)/len(stats_combined)*100:.1f}%)\")\n",
    "\n",
    "# ÎîîÎ≤ÑÍπÖ: ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Î≤îÏúÑ ÌôïÏù∏\n",
    "print(f\"\\nüîç ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Î≤îÏúÑ ÌôïÏù∏:\")\n",
    "print(f\"  Stats History:\")\n",
    "print(f\"    ÏãúÏûë: {stats_combined['timestamp'].min()}\")\n",
    "print(f\"    Ï¢ÖÎ£å: {stats_combined['timestamp'].max()}\")\n",
    "print(f\"  Metrics (phase3_df):\")\n",
    "print(f\"    ÏãúÏûë: {phase3_df['timestamp'].min()}\")\n",
    "print(f\"    Ï¢ÖÎ£å: {phase3_df['timestamp'].max()}\")\n",
    "\n",
    "# Î∞©Î≤ï 1: elapsed_seconds Í∏∞Î∞ò Î≥ëÌï© ÏãúÎèÑ (Îçî ÏïàÏ†ïÏ†Å)\n",
    "print(f\"\\nüîÑ elapsed_seconds Í∏∞Î∞ò Î≥ëÌï© ÏãúÎèÑ...\")\n",
    "\n",
    "# Phase3_dfÏùò elapsed_seconds Ïû¨Í≥ÑÏÇ∞\n",
    "phase3_df_copy = phase3_df.copy()\n",
    "phase3_df_copy['elapsed_seconds'] = (phase3_df_copy['timestamp'] -\n",
    "                                   phase3_df_copy['timestamp'].min()).dt.total_seconds()\n",
    "\n",
    "# elapsed_seconds Í∏∞Î∞ò Î≥ëÌï© (Î≤°ÌÑ∞ Ïó∞ÏÇ∞: merge_asof)\n",
    "phase3_with_failures = pd.merge_asof(\n",
    "  stats_combined[['elapsed_seconds', 'Failures/s', 'Total Failure Count', 'User Count']].sort_values('elapsed_seconds'),\n",
    "  phase3_df_copy.sort_values('elapsed_seconds'),\n",
    "  on='elapsed_seconds',\n",
    "  direction='nearest',\n",
    "  tolerance=5  # 30Ï¥à tolerance\n",
    ")\n",
    "\n",
    "# Failures/sÏôÄ ÏµúÏÜå ÌïòÎÇòÏùò Î©îÌä∏Î¶≠Ïù¥ ÏûàÎäî ÌñâÎßå Ïú†ÏßÄ\n",
    "initial_len = len(phase3_with_failures)\n",
    "phase3_with_failures = phase3_with_failures[\n",
    "  phase3_with_failures['Failures/s'].notna()\n",
    "]\n",
    "\n",
    "print(f\"‚úì MetricsÏôÄ Î≥ëÌï© ÏôÑÎ£å: {len(phase3_with_failures)}Í∞ú ÏÉòÌîå (Ï¥àÍ∏∞: {initial_len}Í∞ú)\")\n",
    "\n",
    "# Failures/sÎ•º ÌÉÄÍ≤üÏúºÎ°ú ÏÑ†Ìñâ ÏßÄÌëú Î∂ÑÏÑù\n",
    "if len(phase3_with_failures) > 0 and phase3_with_failures['Failures/s'].sum() > 0:\n",
    "  target_metric_failures = 'Failures/s'\n",
    "\n",
    "  # ÌõÑÎ≥¥ Î©îÌä∏Î¶≠ (Failures/s Ï†úÏô∏)\n",
    "  candidate_metrics_failures = [\n",
    "      'e2e_latency_p95', 'e2e_latency_p99', 'eventloop_lag_p95', 'eventloop_lag_p99',\n",
    "      'redis_latency_p95', 'redis_backlog', 'broadcast_queue', 'memory_bytes',\n",
    "      'cpu', 'message_rate', 'connected_users', 'error_rate'\n",
    "  ]\n",
    "\n",
    "  # Ï°¥Ïû¨ÌïòÎäî Î©îÌä∏Î¶≠Îßå ÏÑ†ÌÉù\n",
    "  candidate_metrics_failures = [m for m in candidate_metrics_failures\n",
    "                                if m in phase3_with_failures.columns]\n",
    "\n",
    "  print(f\"\\n\" + \"=\" * 80)\n",
    "  print(f\"üîç Failures/s ÏòàÏ∏°ÏùÑ ÏúÑÌïú ÏÑ†Ìñâ ÏßÄÌëú Î∂ÑÏÑù\")\n",
    "  print(\"=\" * 80)\n",
    "  print(f\"ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÌõÑÎ≥¥ Î©îÌä∏Î¶≠: {len(candidate_metrics_failures)}Í∞ú\")\n",
    "\n",
    "  # ÏãúÏ∞® ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í≥ÑÏÇ∞\n",
    "  lags = [0, 5, 10, 15, 20, 30]\n",
    "  corr_matrix_failures = calculate_lagged_correlations(\n",
    "      phase3_with_failures, target_metric_failures, candidate_metrics_failures, lags\n",
    "  )\n",
    "\n",
    "  if corr_matrix_failures is not None and not corr_matrix_failures.empty:\n",
    "      # 1. ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌûàÌä∏Îßµ\n",
    "      plt.figure(figsize=(14, 8))\n",
    "      sns.heatmap(corr_matrix_failures.T, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                 center=0, vmin=-1, vmax=1, cbar_kws={'label': 'Correlation'})\n",
    "      plt.title(f'Phase 3: Failures/s ÏãúÏ∞® ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌûàÌä∏Îßµ\\n(Locust Stats History)',\n",
    "               fontsize=14, fontweight='bold', pad=20)\n",
    "      plt.xlabel('Lag Time (Ï¥à)', fontsize=12)\n",
    "      plt.ylabel('Metrics', fontsize=12)\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "      # 2. Top 3 ÏòàÏ∏° ÏßÄÌëú\n",
    "      top_predictors_failures = find_top_predictors(corr_matrix_failures, top_n=3)\n",
    "\n",
    "      print(f\"\\n‚ú® Top 3 ÏÑ†Ìñâ ÏßÄÌëú (Failures/s ÏòàÏ∏°):\")\n",
    "      for i, pred in enumerate(top_predictors_failures, 1):\n",
    "          direction = \"ÏñëÏùò\" if pred['correlation'] > 0 else \"ÏùåÏùò\"\n",
    "          print(f\"{i}. {pred['metric']}\")\n",
    "          print(f\"   ‚Ä¢ ÏµúÏ†Å ÏÑ†Ìñâ ÏãúÍ∞Ñ: {pred['lag']}Ï¥à\")\n",
    "          print(f\"   ‚Ä¢ ÏÉÅÍ¥ÄÍ≥ÑÏàò: {pred['correlation']:.3f} ({direction} ÏÉÅÍ¥Ä)\")\n",
    "          print(f\"   üí° Ïù∏ÏÇ¨Ïù¥Ìä∏: {pred['metric']}Ïù¥(Í∞Ä) {pred['lag']}Ï¥à Ï†ÑÏóê Î≥ÄÌôîÌïòÎ©¥ \"\n",
    "                f\"ÌÖåÏä§Ìä∏ Ïã§Ìå® Í∞ÄÎä•ÏÑ± ÎÜíÏùå\\n\")\n",
    "\n",
    "      # 3. Top 1 ÏòàÏ∏° ÏßÄÌëú + Failures/s ÏãúÍ∞ÅÌôî\n",
    "      if len(top_predictors_failures) > 0:\n",
    "          top1 = top_predictors_failures[0]\n",
    "\n",
    "          fig, ax1 = plt.subplots(figsize=(16, 6))\n",
    "          ax2 = ax1.twinx()\n",
    "\n",
    "          # ÏÑ†Ìñâ ÏßÄÌëú (ÏãúÌîÑÌä∏ Ï†ÅÏö©)\n",
    "          shifted_predictor = phase3_with_failures[top1['metric']].shift(top1['lag'])\n",
    "\n",
    "          values1 = shifted_predictor / 1_000_000 if 'memory' in top1['metric'] else shifted_predictor\n",
    "          line1 = ax1.plot(phase3_with_failures['elapsed_seconds'], values1,\n",
    "                         color='#f39c12', linewidth=2,\n",
    "                         label=f\"{top1['metric']} (shifted -{top1['lag']}s)\",\n",
    "                         marker='o', markersize=4, alpha=0.7)\n",
    "          ylabel1 = f\"{top1['metric']} (MB)\" if 'memory' in top1['metric'] else top1['metric']\n",
    "          ax1.set_ylabel(ylabel1, fontsize=12, color='#f39c12')\n",
    "          ax1.tick_params(axis='y', labelcolor='#f39c12')\n",
    "\n",
    "          # Failures/s\n",
    "          line2 = ax2.plot(phase3_with_failures['elapsed_seconds'],\n",
    "                         phase3_with_failures[target_metric_failures],\n",
    "                         color='#e74c3c', linewidth=2, label='Failures/s',\n",
    "                         marker='s', markersize=4, alpha=0.7)\n",
    "          ax2.set_ylabel('Failures/s', fontsize=12, color='#e74c3c')\n",
    "          ax2.tick_params(axis='y', labelcolor='#e74c3c')\n",
    "\n",
    "          ax1.set_xlabel('Í≤ΩÍ≥º ÏãúÍ∞Ñ (Ï¥à)', fontsize=12)\n",
    "          ax1.set_title(f'Phase 3: Top 1 ÏÑ†Ìñâ ÏßÄÌëú ({top1[\"metric\"]}) vs Failures/s\\n'\n",
    "                       f'ÏÑ†Ìñâ ÏãúÍ∞Ñ: {top1[\"lag\"]}Ï¥à, ÏÉÅÍ¥ÄÍ≥ÑÏàò: {top1[\"correlation\"]:.3f}',\n",
    "                       fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "          lines = line1 + line2\n",
    "          labels = [l.get_label() for l in lines]\n",
    "          ax1.legend(lines, labels, loc='upper left', fontsize=10)\n",
    "          ax1.grid(True, alpha=0.3)\n",
    "\n",
    "          plt.tight_layout()\n",
    "          plt.show()\n",
    "\n",
    "      print(\"\\n\" + \"=\" * 80)\n",
    "      print(\"üí° Î≤°ÌÑ∞ Ïó∞ÏÇ∞ ÏÇ¨Ïö©:\")\n",
    "      print(\"   ‚Ä¢ pd.concat() - Îã§Ï§ë DataFrame Î≥ëÌï© (O(n))\")\n",
    "      print(\"   ‚Ä¢ pd.merge_asof() - ÏãúÍ≥ÑÏó¥ Í∏∞Î∞ò nearest join (O(n log n))\")\n",
    "      print(\"   ‚Ä¢ elapsed_seconds Í∏∞Î∞ò Î≥ëÌï©ÏúºÎ°ú timestamp ÌòïÏãù Ï∞®Ïù¥ Ìï¥Í≤∞\")\n",
    "      print(\"   ‚Ä¢ df[col].shift() - ÏãúÍ≥ÑÏó¥ ÏãúÌîÑÌä∏ (O(n))\")\n",
    "      print(\"=\" * 80)\n",
    "  else:\n",
    "      print(\"‚ö†Ô∏è  ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í≥ÑÏÇ∞ Ïã§Ìå®\")\n",
    "else:\n",
    "  print(\"\\n‚ö†Ô∏è  Failures/s Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå ÎòêÎäî Î≥ëÌï© Ïã§Ìå®\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_metrics_p3 = ['message_rate', 'e2e_latency_p95', 'eventloop_lag_p95',\n",
    "                          'memory_bytes', 'error_rate']\n",
    "\n",
    "available_metrics_p3 = [m for m in correlation_metrics_p3 if m in phase3_df.columns]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix_metrics_p3 = phase3_df[available_metrics_p3].corr()\n",
    "\n",
    "print(\"Phase 3: Correlation Matrix\")\n",
    "print(\"=\" * 80)\n",
    "print(corr_matrix_metrics_p3.round(3))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix_metrics_p3, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "           center=0, vmin=-1, vmax=1, square=True,\n",
    "           cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Phase 3: Metric Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Strong Correlation Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find strongly correlated pairs\n",
    "strong_corrs_p3 = []\n",
    "for i in range(len(available_metrics_p3)):\n",
    "    for j in range(i+1, len(available_metrics_p3)):\n",
    "        metric1 = available_metrics_p3[i]\n",
    "        metric2 = available_metrics_p3[j]\n",
    "        corr_val = corr_matrix_metrics_p3.loc[metric1, metric2]\n",
    "        \n",
    "        if abs(corr_val) >= 0.7:\n",
    "            strong_corrs_p3.append((metric1, metric2, corr_val))\n",
    "\n",
    "if strong_corrs_p3:\n",
    "    print(f\"Found {len(strong_corrs_p3)} strongly correlated pairs (|r| >= 0.7)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(strong_corrs_p3), 1, \n",
    "                            figsize=(14, 5*len(strong_corrs_p3)))\n",
    "    \n",
    "    if len(strong_corrs_p3) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (m1, m2, corr) in enumerate(strong_corrs_p3):\n",
    "        ax = axes[idx]\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        line1 = ax.plot(phase3_df['elapsed_seconds'], phase3_df[m1], \n",
    "                       'b-', label=m1, linewidth=2)\n",
    "        line2 = ax2.plot(phase3_df['elapsed_seconds'], phase3_df[m2], \n",
    "                        'r-', label=m2, linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Elapsed Time (seconds)', fontsize=11)\n",
    "        ax.set_ylabel(m1, color='b', fontsize=11)\n",
    "        ax2.set_ylabel(m2, color='r', fontsize=11)\n",
    "        ax.set_title(f'{m1} vs {m2} (correlation: {corr:+.3f})', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        \n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax.legend(lines, labels, loc='upper left')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        print(f\"{idx+1}. {m1:30s} <-> {m2:30s} | r = {corr:+.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No strongly correlated pairs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 4: Connection Stress Analysis\n",
    "\n",
    "### Scenario\n",
    "- Concurrent user escalation every 2 minutes: 100 ‚Üí 200 ‚Üí 300 ‚Üí 400 ‚Üí 500 ‚Üí 600\n",
    "- Objective: Identify connection limit where system scalability degrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase4_df = phase_data['phase4_connection']\n",
    "\n",
    "# Visualize safe zone violations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, label) in enumerate(CORE_METRICS):\n",
    "    if metric not in phase4_df.columns:\n",
    "        continue\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Convert memory to MB if needed\n",
    "    y_data = phase4_df[metric] / (1024*1024) if metric == 'memory_bytes' else phase4_df[metric]\n",
    "    safe_zone = safe_zones[metric] / (1024*1024) if metric == 'memory_bytes' else safe_zones[metric]\n",
    "    \n",
    "    # Plot metric\n",
    "    ax.plot(phase4_df['elapsed_seconds'], y_data, \n",
    "           label=f'{label}', color='#e74c3c', linewidth=2)\n",
    "    \n",
    "    # Safe zone threshold\n",
    "    ax.axhline(y=safe_zone, color='red', linestyle='--', linewidth=2,\n",
    "              label=f'Safe Zone Upper ({safe_zone:.1f})')\n",
    "    \n",
    "    # Highlight violations\n",
    "    violations = y_data > safe_zone\n",
    "    if violations.any():\n",
    "        ax.fill_between(phase4_df['elapsed_seconds'], y_data, safe_zone,\n",
    "                        where=violations, alpha=0.3, color='red',\n",
    "                        label='Violation Zone')\n",
    "    \n",
    "    ax.set_xlabel('Elapsed Time (seconds)', fontsize=10)\n",
    "    ax.set_ylabel(label, fontsize=10)\n",
    "    ax.set_title(f'Phase 4: {label} vs Safe Zone', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Phase 4 (Connection Stress): Safe Zone Violations', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: User Count Analysis at Violation Points\n",
    "\n",
    "Analyze concurrent user count when E2E Latency P95 and Event Loop Lag P95 violate safe zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple metric violations for Phase 4\n",
    "metrics_to_check_p4 = [\n",
    "    ('e2e_latency_p95', 'E2E Latency P95'),\n",
    "    ('eventloop_lag_p95', 'Event Loop Lag P95')\n",
    "]\n",
    "\n",
    "print(\"Phase 4: Multi-Metric Violation Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric, label in metrics_to_check_p4:\n",
    "    if metric not in phase4_df.columns or metric not in safe_zones:\n",
    "        continue\n",
    "    \n",
    "    violations = identify_violations(phase4_df, metric, safe_zones[metric])\n",
    "    \n",
    "    if len(violations) > 0:\n",
    "        first_violation = violations.iloc[0]\n",
    "        violation_time = first_violation['elapsed_seconds']\n",
    "        violation_value = first_violation[metric]\n",
    "        user_count = get_user_count_phase4(violation_time)\n",
    "        \n",
    "        print(f\"\\n{label}:\")\n",
    "        print(f\"  Safe Zone Upper: {safe_zones[metric]:.2f} ms\")\n",
    "        print(f\"  First Violation: {violation_time:.1f}s at {violation_value:.2f} ms\")\n",
    "        print(f\"  Concurrent Users: {user_count}\")\n",
    "        print(f\"  Total Violations: {len(violations)}\")\n",
    "    else:\n",
    "        print(f\"\\n{label}: No violations detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Leading Indicator Lagged Correlation Analysis\n",
    "\n",
    "Find leading indicators for connection stress. Since Phase 4 has minimal errors, \n",
    "we use e2e_latency_p95 as the target metric instead of error_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4 leading indicator analysis\n",
    "# Use e2e_latency_p95 as target since error_rate is minimal\n",
    "target_metric_p4 = 'e2e_latency_p95'\n",
    "\n",
    "candidate_metrics_p4 = [\n",
    "    col for col in phase4_df.select_dtypes(include=[np.number]).columns\n",
    "    if col not in ['e2e_latency_p95', 'elapsed_seconds', 'timestamp']\n",
    "]\n",
    "\n",
    "# Calculate lagged correlations\n",
    "corr_matrix_p4 = calculate_lagged_correlations(\n",
    "    phase4_df, target_metric_p4, candidate_metrics_p4, \n",
    "    lags=[0, 5, 10, 15, 20, 30]\n",
    ")\n",
    "\n",
    "print(\"Phase 4: Top 10 Metrics with Lagged Correlation to E2E Latency P95\")\n",
    "print(\"=\" * 80)\n",
    "print(corr_matrix_p4.head(10).round(3))\n",
    "\n",
    "# Find top leading indicators\n",
    "top_indicators_p4 = find_top_leading_indicators(corr_matrix_p4, top_n=3)\n",
    "\n",
    "print(f\"\\n\\nTop 3 Leading Indicators for E2E Latency P95:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (metric, lag, corr) in enumerate(top_indicators_p4, 1):\n",
    "    print(f\"{i}. {metric:30s} | Optimal Lag: {lag:2d}s | Correlation: {corr:+.3f}\")\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plot_correlation_heatmap(corr_matrix_p4.head(15), \n",
    "                        title=\"Phase 4: Lagged Correlation with E2E Latency P95 (Top 15 Metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_metrics_p4 = ['connected_users', 'e2e_latency_p95', 'eventloop_lag_p95',\n",
    "                          'memory_bytes', 'ws_send_latency_p95']\n",
    "\n",
    "available_metrics_p4 = [m for m in correlation_metrics_p4 if m in phase4_df.columns]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix_metrics_p4 = phase4_df[available_metrics_p4].corr()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix_metrics_p4, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "           center=0, vmin=-1, vmax=1, square=True,\n",
    "           cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Phase 4: Metric Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Strong Correlation Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find strongly correlated pairs\n",
    "strong_corrs_p4 = []\n",
    "for i in range(len(available_metrics_p4)):\n",
    "    for j in range(i+1, len(available_metrics_p4)):\n",
    "        metric1 = available_metrics_p4[i]\n",
    "        metric2 = available_metrics_p4[j]\n",
    "        corr_val = corr_matrix_metrics_p4.loc[metric1, metric2]\n",
    "        \n",
    "        if abs(corr_val) >= 0.7:\n",
    "            strong_corrs_p4.append((metric1, metric2, corr_val))\n",
    "\n",
    "if strong_corrs_p4:\n",
    "    print(f\"Found {len(strong_corrs_p4)} strongly correlated pairs (|r| >= 0.7)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(strong_corrs_p4), 1, \n",
    "                            figsize=(14, 5*len(strong_corrs_p4)))\n",
    "    \n",
    "    if len(strong_corrs_p4) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (m1, m2, corr) in enumerate(strong_corrs_p4):\n",
    "        ax = axes[idx]\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        line1 = ax.plot(phase4_df['elapsed_seconds'], phase4_df[m1], \n",
    "                       'b-', label=m1, linewidth=2)\n",
    "        line2 = ax2.plot(phase4_df['elapsed_seconds'], phase4_df[m2], \n",
    "                        'r-', label=m2, linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel('Elapsed Time (seconds)', fontsize=11)\n",
    "        ax.set_ylabel(m1, color='b', fontsize=11)\n",
    "        ax2.set_ylabel(m2, color='r', fontsize=11)\n",
    "        ax.set_title(f'{m1} vs {m2} (correlation: {corr:+.3f})', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        \n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax.legend(lines, labels, loc='upper left')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        print(f\"{idx+1}. {m1:30s} <-> {m2:30s} | r = {corr:+.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No strongly correlated pairs found\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
